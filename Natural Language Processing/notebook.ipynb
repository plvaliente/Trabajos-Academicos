{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Informe ECI 2019 - Introducción al Lenguaje Natural con Redes Neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este trabajo decidimos utilizar fastText para reproducir los resultados del paper. fastText es una libreria de clasificación y representación de texto. Lo realiza transformando el texto en vectores continuos.\n",
    "\n",
    "Encontramos buenos resultados con los parametros default de fastText, pero realizaremos una busqueda de grilla (Grid Search) para encontrar los que den mejores resultados.\n",
    "\n",
    "En esta notebook se pueden generar los archivos de train, dev y test, correr una validación contra dev y generar el archivo de predicciones de test en el formato de Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import csv\n",
    "import argparse\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import math\n",
    "sns.set()\n",
    "plt.rcParams['figure.figsize'] = (10,7)\n",
    "plt.rcParams['font.size'] = 23\n",
    "sns.set_context('paper', font_scale=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones auxiliares para parsear texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import argparse\n",
    "import json\n",
    "import csv\n",
    "\n",
    "def generate_file(sentences_file, label_file, output):\n",
    "    if label_file != None:\n",
    "        for sentence, label in zip(it_sentences(sentences_file), it_labels(label_file)):\n",
    "        \toutput.write(\"\".join([\"__label__\",label,\" \",sentence,\" \",\"\\n\"]))\n",
    "            # Tenemos la oración en sentence con su categoría en label\n",
    "            #print(label, sentence)\n",
    "    else:\n",
    "        for sentence in it_sentences(sentences_file):\n",
    "            output.write(\"%s\\n\" % sentence)\n",
    "            # Tenemos una oración en sentence\n",
    "            #print(sentence)\n",
    "            pass\n",
    "    \n",
    "\n",
    "def it_sentences(sentence_data):\n",
    "    for line in sentence_data:\n",
    "        example = json.loads(line)\n",
    "        yield example['sentence2']\n",
    "\n",
    "def it_labels(label_data):\n",
    "    label_data_reader = csv.DictReader(label_data)\n",
    "    for example in label_data_reader:\n",
    "        yield example['gold_label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generamos los files de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate train data\n",
    "generate_file(open(\"snli_1.0_train_filtered.jsonl\", \"r\"), open(\"snli_1.0_train_gold_labels.csv\", \"r\"), open(\"fast.txt\", \"w+\"))\n",
    "#Generate dev data\n",
    "generate_file(open(\"snli_1.0_dev_filtered.jsonl\", \"r\"), open(\"snli_1.0_dev_gold_labels.csv\", \"r\"), open(\"fast_dev.txt\", \"w+\"))\n",
    "#Generate test data\n",
    "generate_file(open(\"snli_1.0_test_filtered.jsonl\", \"r\"), None, open(\"fast_test.txt\", \"w+\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Busqueda de parametros con Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En nuestro Grid Search vamos a considerar diferentes parametros que consideramos que pueden dar resultados interesantes: el learning rate, los epochs, el tamaño de la ventana de contexto de palabras, la función de perdida, el máximo tamaño de los ngrams y el tamaño del vector de palabras. Utilizando los parametros default de fastText conseguimos los siguientes resultados comparando contra el dataset de dev:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fastText usa como parametros default:\n",
    "* learning rate: 0.1\n",
    "* epoch: 5\n",
    "* window: 5\n",
    "* loss function: softmax\n",
    "* nGram: 1\n",
    "* dims: 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El porcentaje de aciertos es: 65.06807559439139\n"
     ]
    }
   ],
   "source": [
    "def get_score(model):\n",
    "    dev_data = open(\"fast_dev.txt\", \"r\")\n",
    "    result = 0\n",
    "    total = 0\n",
    "    for line in dev_data:\n",
    "        split = line.split(\" \", 1)\n",
    "        result += model.predict(split[1].rstrip('\\n'))[0][0][len(\"__label__\"):] == split[0][len(\"__label__\"):]\n",
    "        total += 1\n",
    "    return result*100/total\n",
    "\n",
    "model = fasttext.train_supervised(\"fast.txt\")\n",
    "print(\"El porcentaje de aciertos es: %s\" % get_score(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con estos parametros no logramos replicar los resultados obtenidos en el paper. Esto nos motiva a realizar nuestro Grid Search sobre parametros que nos dieron buenos resultados en pruebas preliminares y que nos resultaron interesantes para experimentar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para correr el Grid Search (tarda aproximadamente 8 horas, adjuntamos el csv con los resultados):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = [0.01, 0.1, 0.25]\n",
    "epochs = [5, 10, 100]\n",
    "windows = [2, 5, 7]\n",
    "loss = [\"ova\", \"softmax\"]\n",
    "wordNgrams = [1, 2, 3]\n",
    "dims = [100, 300]\n",
    "\n",
    "for lr in learning_rate:\n",
    "    for epoch in epochs:\n",
    "        for ws in windows:\n",
    "            for l in loss:\n",
    "                for nGram in wordNgrams:\n",
    "                    for dim in dims:\n",
    "                        dev_data = open(\"fast_dev.txt\", \"r\")\n",
    "                        model = fasttext.train_supervised(\"fast.txt\", lr = lr, epoch= epoch, ws= ws, loss=l, wordNgrams = nGram, dim=dim)\n",
    "                        result = 0\n",
    "                        total = 0\n",
    "                        for line in dev_data:\n",
    "                            split = line.split(\" \", 1)\n",
    "                            result += model.predict(split[1].rstrip('\\n'))[0][0][len(\"__label__\"):] == split[0][len(\"__label__\"):]\n",
    "                            total += 1\n",
    "                        print(\"lr:%s, epoch:%s, window:%s, loss=%s, nGram=%s, dim:%s\" % (lr, epoch, ws, l, nGram, dim))\n",
    "                        print(result*100/total)\n",
    "                        del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encontramos multiples combinaciones de parametros que logran tener una accuracy de 67% sobre el set de datos de validación, con ligeras variaciones entre si. Aun así no logramos ningun resultado superador al propuesto por el paper, por lo que podemos considerar que 67% es el mejor resultado para fastText. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas cosas que pudimos ver a partir de los resultados son:\n",
    "\n",
    "* Si el learning rate va a ser bajo, necesita ser acompañado de muchos epochs para obtener buenos resultados. Esto parece dar buenos resultados de forma más consistente.\n",
    "* No podamos dar ninguna afirmación sobre los otros parametros sobre los que experimentamos: conseguimos resultados aceptables con combinaciones de todos ellos, ten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despues de correr contra el dataset de test conseguimos resultados similares, pudiendo replicar los resultados esperados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generación de files de test en formato de Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para conseguir un modelo bueno, nos vamos a quedar con el que mejor desempeño tenga contra el set de validación, utilizando los parametros que mejor resultados nos dieron en el experimento anterior. El que tenga mejores resultados sera enviado como nuestra respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.39483844747002\n",
      "67.40499898394636\n",
      "67.41515952042268\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.train_supervised(\"fast.txt\", lr = 0.001, epoch= 100, ws= 2, loss=\"ova\", wordNgrams = 3, dim=300)\n",
    "best_score = get_score(model)\n",
    "for i in range(0, 10):\n",
    "    new_model = fasttext.train_supervised(\"fast.txt\", lr = 0.001, epoch= 100, ws= 2, loss=\"ova\", wordNgrams = 3, dim=300)\n",
    "    score = get_score(new_model)\n",
    "    if score > best_score:\n",
    "        print(score)\n",
    "        best_score = score\n",
    "        del model\n",
    "        model = new_model\n",
    "    else:\n",
    "        del new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer():\n",
    "    \"Junta el archivo con las oraciones de test (jsonl)\"\n",
    "    \" y los resultados de la clasificación de tu algoritmo (en tu formato)\"\n",
    "    \" en un archivo csv compatible con el formato de Kaggle\"\n",
    "\n",
    "    sentences_filename = \"snli_1.0_test_filtered.jsonl\"\n",
    "    labels_filename = \"test_cls.txt\"\n",
    "    output_filename = \"result.csv\"\n",
    "\n",
    "    with open(output_filename, 'w+') as fout:\n",
    "        csv_writer = csv.writer(fout)\n",
    "        csv_writer.writerow(['pairID', 'gold_label'])\n",
    "\n",
    "        for pairID, label in it_ID_label_pairs(sentences_filename, labels_filename):\n",
    "            formatted_label = label\n",
    "            csv_writer.writerow([pairID, formatted_label])\n",
    "\n",
    "def format_label(label):\n",
    "    return label[len(\"__label__\"):]\n",
    "\n",
    "def it_ID_label_pairs(sentences_filename, labels_filename):\n",
    "    sentence_data = open(sentences_filename, 'r')\n",
    "    labels_data = open(labels_filename, 'r')\n",
    "    for pairID, label in zip(it_ID(sentence_data), it_labels(labels_data)):\n",
    "        yield pairID, label\n",
    "\n",
    "def it_ID(sentence_data):\n",
    "    for line in sentence_data:\n",
    "        example = json.loads(line)\n",
    "        yield example['pairID']\n",
    "\n",
    "def it_labels(label_data):\n",
    "    for label in label_data:\n",
    "        label = label.rstrip('\\n')  # sacamos el fin de linea\n",
    "        yield label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Genera el file de test\n",
    "output = open(\"test_cls.txt\", \"w+\")\n",
    "test_data = open(\"fast_test.txt\", \"r\")\n",
    "for line in test_data:\n",
    "    output.write(\"%s\\n\" % model.predict(line.rstrip('\\n'))[0][0][len(\"__label__\"):])\n",
    "    \n",
    "generate_answer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
